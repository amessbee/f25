{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed543d92",
   "metadata": {},
   "source": [
    "# Social Network Analysis I - Week 1\n",
    "## Python Fundamentals & Data Handling\n",
    "\n",
    "**Instructor:** Dr. Mudassir Shabbir  \n",
    "**Fall 2025**\n",
    "\n",
    "---\n",
    "\n",
    "### Course Overview\n",
    "- **Level:** 300-level undergraduate  \n",
    "- **Credits:** 3  \n",
    "- **Prerequisite:** Algorithms course with grade B- or higher  \n",
    "- **Format:** Hands-on lectures, labs, homeworks, group activities, and final project\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this course, you will:\n",
    "1. Understand core concepts in supervised and unsupervised ML\n",
    "2. Gain proficiency in Python for ML and data analysis\n",
    "3. Model, analyze, and visualize real-world networks\n",
    "4. Apply ML techniques to network problems (link prediction, node classification)\n",
    "5. Collaborate on applied projects and communicate results\n",
    "\n",
    "### Today's Focus\n",
    "Building the foundation: Python skills and data handling with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3dd3c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Imports\n",
    "\n",
    "### Development Environment Options\n",
    "- **Google Colab (Recommended):** Free, cloud-based, pre-installed libraries, easy sharing\n",
    "- **Local Jupyter Notebooks:** Full control, works offline, better for large datasets\n",
    "\n",
    "### Getting Started with Colab\n",
    "1. Go to https://colab.research.google.com\n",
    "2. Sign in with Google account\n",
    "3. Create new notebook\n",
    "4. Start coding!\n",
    "\n",
    "Let's import the essential libraries we'll use throughout this course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fadb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Python Libraries for Social Network Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2491c",
   "metadata": {},
   "source": [
    "## 2. Python Basics Review\n",
    "\n",
    "Let's refresh some fundamental Python concepts that are essential for data analysis and network analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e65419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data types\n",
    "number = 42\n",
    "text = \"Social Network Analysis\"\n",
    "is_fun = True\n",
    "\n",
    "print(f\"Number: {number} (type: {type(number)})\")\n",
    "print(f\"Text: {text} (type: {type(text)})\")\n",
    "print(f\"Boolean: {is_fun} (type: {type(is_fun)})\")\n",
    "\n",
    "# Lists and dictionaries\n",
    "students = [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"]\n",
    "grades = {\"Alice\": 95, \"Bob\": 87, \"Charlie\": 92, \"Diana\": 78, \"Eve\": 88}\n",
    "\n",
    "print(f\"\\nStudents: {students}\")\n",
    "print(f\"Grades: {grades}\")\n",
    "\n",
    "# List comprehensions (very useful for data processing!)\n",
    "squares = [x**2 for x in range(1, 6)]\n",
    "high_grades = [name for name, grade in grades.items() if grade > 90]\n",
    "\n",
    "print(f\"\\nSquares: {squares}\")\n",
    "print(f\"High achievers: {high_grades}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce861b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions and control flow - essential for network analysis\n",
    "def analyze_network_stats(connections):\n",
    "    \"\"\"Calculate basic network statistics from a list of connections.\"\"\"\n",
    "    if not connections:\n",
    "        return \"No connections found\"\n",
    "    \n",
    "    total_connections = len(connections)\n",
    "    unique_users = len(set([user for conn in connections for user in conn.split('-')]))\n",
    "    \n",
    "    # Calculate density (simplified metric)\n",
    "    max_possible_connections = unique_users * (unique_users - 1) / 2\n",
    "    density = total_connections / max_possible_connections if max_possible_connections > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_connections': total_connections,\n",
    "        'unique_users': unique_users,\n",
    "        'network_density': round(density, 3)\n",
    "    }\n",
    "\n",
    "# Example usage with social network data\n",
    "connections = ['Alice-Bob', 'Bob-Charlie', 'Alice-Charlie', 'Diana-Eve', 'Charlie-Diana']\n",
    "stats = analyze_network_stats(connections)\n",
    "\n",
    "print(\"Network Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Demonstrate control flow\n",
    "print(f\"\\nNetwork Analysis:\")\n",
    "if stats['network_density'] > 0.5:\n",
    "    print(\"  üîó Dense network - high connectivity\")\n",
    "elif stats['network_density'] > 0.3:\n",
    "    print(\"  üîó Medium density network\")\n",
    "else:\n",
    "    print(\"  üîó Sparse network - low connectivity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935ca81",
   "metadata": {},
   "source": [
    "## 3. Creating and Exploring DataFrames\n",
    "\n",
    "**Why Pandas for Network Analysis?**\n",
    "- **Data Import:** Read CSV, JSON, Excel files easily\n",
    "- **Data Cleaning:** Handle missing values, duplicates, inconsistencies  \n",
    "- **Data Transformation:** Filter, group, aggregate, pivot\n",
    "- **Network Representation:** Edge lists, adjacency matrices\n",
    "- **Integration:** Works seamlessly with NetworkX, scikit-learn\n",
    "\n",
    "**Key Pandas Objects:**\n",
    "- **Series:** 1D labeled array (like a column)\n",
    "- **DataFrame:** 2D labeled data structure (like a spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrames - Different approaches\n",
    "\n",
    "# Method 1: From dictionary (most common for small datasets)\n",
    "social_data = {\n",
    "    'user_id': [1, 2, 3, 4, 5],\n",
    "    'username': ['alice', 'bob', 'charlie', 'diana', 'eve'],\n",
    "    'followers': [150, 89, 203, 45, 312],\n",
    "    'following': [98, 156, 87, 178, 92],\n",
    "    'posts': [45, 23, 67, 12, 89]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(social_data)\n",
    "print(\"üìä Social Network User Data\")\n",
    "print(\"=\" * 40)\n",
    "print(df)\n",
    "\n",
    "print(f\"\\nüìè DataFrame shape: {df.shape} (rows, columns)\")\n",
    "print(f\"üìã Column names: {list(df.columns)}\")\n",
    "print(f\"üî¢ Data types:\\n{df.dtypes}\")\n",
    "\n",
    "# Method 2: From lists of lists\n",
    "user_data = [\n",
    "    [6, 'frank', 78, 145, 34],\n",
    "    [7, 'grace', 234, 67, 78],\n",
    "    [8, 'henry', 156, 189, 56]\n",
    "]\n",
    "\n",
    "additional_df = pd.DataFrame(user_data, columns=['user_id', 'username', 'followers', 'following', 'posts'])\n",
    "print(f\"\\nüìä Additional Users:\")\n",
    "print(additional_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ecc542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential DataFrame exploration methods\n",
    "\n",
    "print(\"üîç EXPLORING THE DATAFRAME\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Basic info about the dataset\n",
    "print(\"üìã DataFrame Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# 2. Statistical summary\n",
    "print(\"üìä Statistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# 3. First and last few rows\n",
    "print(\"üëÜ First 3 rows:\")\n",
    "print(df.head(3))\n",
    "\n",
    "print(\"\\nüëá Last 2 rows:\")\n",
    "print(df.tail(2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# 4. Check for missing values\n",
    "print(\"‚ùì Missing values check:\")\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data)\n",
    "\n",
    "if missing_data.sum() == 0:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# 5. Quick insights\n",
    "print(\"üí° Quick Insights:\")\n",
    "print(f\"  üìà Average followers: {df['followers'].mean():.1f}\")\n",
    "print(f\"  üìä Most active user: {df.loc[df['posts'].idxmax(), 'username']}\")\n",
    "print(f\"  üèÜ Most followed user: {df.loc[df['followers'].idxmax(), 'username']}\")\n",
    "print(f\"  üì± Total posts across all users: {df['posts'].sum()}\")\n",
    "print(f\"  üë• Average following: {df['following'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec096c7f",
   "metadata": {},
   "source": [
    "## 4. Data Selection and Filtering\n",
    "\n",
    "One of the most important skills in data analysis is being able to select and filter data efficiently. Let's practice different selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d623c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Selection and Filtering Techniques\n",
    "\n",
    "print(\"üéØ DATA SELECTION TECHNIQUES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Select specific columns\n",
    "print(\"üìã Select specific columns:\")\n",
    "user_metrics = df[['username', 'followers', 'posts']]\n",
    "print(user_metrics)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# 2. Single column selection (returns Series)\n",
    "print(\"üë§ Usernames only (Series):\")\n",
    "usernames = df['username']\n",
    "print(usernames)\n",
    "print(f\"Type: {type(usernames)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# 3. Filter rows based on conditions\n",
    "print(\"üèÜ Popular users (followers > 100):\")\n",
    "popular_users = df[df['followers'] > 100]\n",
    "print(popular_users[['username', 'followers']])\n",
    "\n",
    "print(\"\\nüì± Active users (posts > 50):\")\n",
    "active_users = df[df['posts'] > 50]\n",
    "print(active_users[['username', 'posts']])\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# 4. Multiple conditions with & (and) and | (or)\n",
    "print(\"‚≠ê Influential users (followers > 100 AND posts > 30):\")\n",
    "influential = df[(df['followers'] > 100) & (df['posts'] > 30)]\n",
    "print(influential[['username', 'followers', 'posts']])\n",
    "\n",
    "print(\"\\nüöÄ Super active OR popular (posts > 60 OR followers > 200):\")\n",
    "super_users = df[(df['posts'] > 60) | (df['followers'] > 200)]\n",
    "print(super_users[['username', 'followers', 'posts']])\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# 5. Using .loc and .iloc for specific selections\n",
    "print(\"üéØ Using .loc (label-based selection):\")\n",
    "# Select specific user by condition\n",
    "top_user = df.loc[df['followers'].idxmax(), ['username', 'followers']]\n",
    "print(f\"Most followed user: {top_user['username']} with {top_user['followers']} followers\")\n",
    "\n",
    "print(\"\\nüî¢ Using .iloc (position-based selection):\")\n",
    "# Select first 3 rows and first 3 columns\n",
    "subset = df.iloc[0:3, 0:3]\n",
    "print(subset)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# 6. String operations on text columns\n",
    "print(\"üî§ String filtering:\")\n",
    "users_with_a = df[df['username'].str.contains('a')]\n",
    "print(\"Users with 'a' in username:\")\n",
    "print(users_with_a[['username']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52fef9f",
   "metadata": {},
   "source": [
    "## 5. Handling Missing Data\n",
    "\n",
    "Real-world data often has missing values. Let's learn how to identify and handle them appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593606d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with missing values to demonstrate handling techniques\n",
    "\n",
    "# Simulate messy real-world data\n",
    "messy_data = {\n",
    "    'user_id': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'username': ['alice', 'bob', None, 'diana', 'eve', 'frank', 'grace', 'henry'],\n",
    "    'followers': [150, np.nan, 203, 45, 312, np.nan, 234, 156],\n",
    "    'following': [98, 156, 87, np.nan, 92, 145, 67, 189],\n",
    "    'posts': [45, 23, 67, 12, np.nan, 34, 78, 56],\n",
    "    'engagement_rate': [0.05, 0.03, np.nan, 0.08, 0.04, 0.06, np.nan, 0.07]\n",
    "}\n",
    "\n",
    "messy_df = pd.DataFrame(messy_data)\n",
    "\n",
    "print(\"üö® MESSY DATA WITH MISSING VALUES\")\n",
    "print(\"=\" * 50)\n",
    "print(messy_df)\n",
    "\n",
    "print(f\"\\nüìä Missing data overview:\")\n",
    "missing_summary = messy_df.isnull().sum()\n",
    "print(missing_summary)\n",
    "\n",
    "print(f\"\\nüìà Missing data percentage:\")\n",
    "missing_percentage = (messy_df.isnull().sum() / len(messy_df) * 100).round(2)\n",
    "for col, pct in missing_percentage.items():\n",
    "    if pct > 0:\n",
    "        print(f\"  {col}: {pct}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Different strategies for handling missing data\n",
    "\n",
    "# Strategy 1: Drop rows with ANY missing values\n",
    "print(\"üóëÔ∏è Strategy 1: Drop rows with ANY missing values\")\n",
    "clean_df_dropall = messy_df.dropna()\n",
    "print(f\"Original shape: {messy_df.shape}, After dropping: {clean_df_dropall.shape}\")\n",
    "print(clean_df_dropall)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# Strategy 2: Drop rows only if ALL values are missing\n",
    "print(\"üóëÔ∏è Strategy 2: Drop rows only if ALL values are missing\")\n",
    "clean_df_dropall_missing = messy_df.dropna(how='all')\n",
    "print(f\"Shape after dropping all-missing rows: {clean_df_dropall_missing.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# Strategy 3: Fill missing values with mean (for numerical columns)\n",
    "print(\"üî¢ Strategy 3: Fill numerical missing values with mean\")\n",
    "filled_df = messy_df.copy()\n",
    "numerical_cols = ['followers', 'following', 'posts', 'engagement_rate']\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if col in filled_df.columns:\n",
    "        mean_value = filled_df[col].mean()\n",
    "        filled_df[col].fillna(mean_value, inplace=True)\n",
    "        print(f\"  Filled {col} missing values with mean: {mean_value:.2f}\")\n",
    "\n",
    "print(\"\\nAfter filling numerical columns:\")\n",
    "print(filled_df)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# Strategy 4: Fill text missing values with a placeholder\n",
    "print(\"üìù Strategy 4: Fill text missing values\")\n",
    "filled_df['username'].fillna('unknown_user', inplace=True)\n",
    "print(\"After filling username:\")\n",
    "print(filled_df[['user_id', 'username']])\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# Strategy 5: Forward fill (useful for time series)\n",
    "print(\"‚è≠Ô∏è Strategy 5: Forward fill example\")\n",
    "time_series_data = pd.DataFrame({\n",
    "    'date': pd.date_range('2025-01-01', periods=6, freq='D'),\n",
    "    'user_count': [100, np.nan, np.nan, 120, np.nan, 140]\n",
    "})\n",
    "print(\"Original time series:\")\n",
    "print(time_series_data)\n",
    "\n",
    "time_series_data['user_count_ffill'] = time_series_data['user_count'].fillna(method='ffill')\n",
    "print(\"\\nAfter forward fill:\")\n",
    "print(time_series_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7a9590",
   "metadata": {},
   "source": [
    "## 6. Data Type Conversion and Validation\n",
    "\n",
    "Ensuring correct data types is crucial for analysis. Let's practice converting and validating data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Type Conversion and Validation Examples\n",
    "\n",
    "# Create sample data with wrong data types (common in real datasets)\n",
    "network_events = pd.DataFrame({\n",
    "    'event_date': ['2025-01-15', '2025-01-16', '2025-01-17', '2025-01-18', '2025-01-19'],\n",
    "    'user_id': ['123', '456', '789', '101', '202'],  # Should be integers\n",
    "    'event_type': ['like', 'share', 'comment', 'follow', 'like'],\n",
    "    'timestamp': ['09:30:00', '14:25:30', '18:45:15', '11:20:45', '16:10:30'],\n",
    "    'value': ['1.5', '2.3', '0.8', '3.1', '1.9']  # Should be float\n",
    "})\n",
    "\n",
    "print(\"üîç ORIGINAL DATA TYPES\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Data:\")\n",
    "print(network_events)\n",
    "print(f\"\\nData types:\")\n",
    "print(network_events.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Convert data types\n",
    "print(\"üîÑ CONVERTING DATA TYPES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 1. Convert string dates to datetime\n",
    "network_events['event_date'] = pd.to_datetime(network_events['event_date'])\n",
    "print(\"‚úÖ Converted event_date to datetime\")\n",
    "\n",
    "# 2. Convert string user_id to integer\n",
    "network_events['user_id'] = network_events['user_id'].astype(int)\n",
    "print(\"‚úÖ Converted user_id to integer\")\n",
    "\n",
    "# 3. Convert string values to float\n",
    "network_events['value'] = network_events['value'].astype(float)\n",
    "print(\"‚úÖ Converted value to float\")\n",
    "\n",
    "# 4. Convert event_type to category (saves memory for repeated values)\n",
    "network_events['event_type'] = network_events['event_type'].astype('category')\n",
    "print(\"‚úÖ Converted event_type to category\")\n",
    "\n",
    "print(f\"\\nData types after conversion:\")\n",
    "print(network_events.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# Validate the conversions\n",
    "print(\"‚úÖ VALIDATION RESULTS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check if dates are valid\n",
    "print(f\"Date range: {network_events['event_date'].min()} to {network_events['event_date'].max()}\")\n",
    "\n",
    "# Check if user_ids are positive integers\n",
    "print(f\"User ID range: {network_events['user_id'].min()} to {network_events['user_id'].max()}\")\n",
    "\n",
    "# Check value statistics\n",
    "print(f\"Value statistics: min={network_events['value'].min()}, max={network_events['value'].max()}, mean={network_events['value'].mean():.2f}\")\n",
    "\n",
    "# Check categories\n",
    "print(f\"Event types: {list(network_events['event_type'].cat.categories)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Advanced: Create datetime from separate date and time columns\n",
    "print(\"üïê ADVANCED: Combining date and time\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Combine date and time into full datetime\n",
    "network_events['full_datetime'] = pd.to_datetime(\n",
    "    network_events['event_date'].dt.strftime('%Y-%m-%d') + ' ' + network_events['timestamp']\n",
    ")\n",
    "\n",
    "print(\"Combined datetime column:\")\n",
    "print(network_events[['event_date', 'timestamp', 'full_datetime']].head())\n",
    "\n",
    "# Extract useful datetime components\n",
    "network_events['hour'] = network_events['full_datetime'].dt.hour\n",
    "network_events['day_of_week'] = network_events['full_datetime'].dt.day_name()\n",
    "\n",
    "print(f\"\\nEvent distribution by hour:\")\n",
    "print(network_events['hour'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nFinal dataset:\")\n",
    "print(network_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e356bdb4",
   "metadata": {},
   "source": [
    "## 7. Removing Duplicates and Outliers\n",
    "\n",
    "Data quality is crucial for network analysis. Let's learn to identify and handle duplicates and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda4f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Duplicates and Outliers\n",
    "\n",
    "print(\"üîç HANDLING DUPLICATES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample interaction data with duplicates\n",
    "user_interactions = pd.DataFrame({\n",
    "    'from_user': ['alice', 'bob', 'alice', 'charlie', 'alice', 'bob', 'diana', 'alice'],\n",
    "    'to_user': ['bob', 'charlie', 'bob', 'alice', 'diana', 'charlie', 'eve', 'bob'],\n",
    "    'interaction_type': ['like', 'comment', 'like', 'share', 'follow', 'comment', 'like', 'like'],\n",
    "    'timestamp': pd.date_range('2025-01-01', periods=8, freq='H')\n",
    "})\n",
    "\n",
    "print(\"Original interaction data:\")\n",
    "print(user_interactions)\n",
    "print(f\"Shape: {user_interactions.shape}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate rows: {user_interactions.duplicated().sum()}\")\n",
    "print(\"Duplicate rows details:\")\n",
    "print(user_interactions[user_interactions.duplicated(keep=False)])\n",
    "\n",
    "# Remove duplicates\n",
    "unique_interactions = user_interactions.drop_duplicates()\n",
    "print(f\"\\nAfter removing duplicates:\")\n",
    "print(f\"Shape: {unique_interactions.shape}\")\n",
    "print(unique_interactions)\n",
    "\n",
    "# Remove duplicates based on specific columns only\n",
    "unique_connections = user_interactions.drop_duplicates(subset=['from_user', 'to_user'], keep='first')\n",
    "print(f\"\\nUnique connections (ignoring interaction type and time):\")\n",
    "print(f\"Shape: {unique_connections.shape}\")\n",
    "print(unique_connections)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# HANDLING OUTLIERS\n",
    "print(\"üìä HANDLING OUTLIERS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create sample data with outliers\n",
    "user_stats = pd.DataFrame({\n",
    "    'user_id': range(1, 21),\n",
    "    'followers': [150, 89, 203, 45, 312, 78, 156, 234, 67, 189,\n",
    "                  145, 298, 56, 187, 123, 5000, 245, 167, 89, 278],  # 5000 is an outlier\n",
    "    'posts': [45, 23, 67, 12, 89, 34, 56, 78, 29, 67,\n",
    "              43, 87, 23, 65, 45, 2000, 56, 34, 28, 71]  # 2000 is an outlier\n",
    "})\n",
    "\n",
    "print(\"User statistics with potential outliers:\")\n",
    "print(user_stats.describe())\n",
    "\n",
    "# Method 1: IQR (Interquartile Range) method for outlier detection\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Detect outliers in followers\n",
    "follower_outliers, lower_followers, upper_followers = detect_outliers_iqr(user_stats, 'followers')\n",
    "print(f\"\\nüîç Followers outlier detection:\")\n",
    "print(f\"Valid range: {lower_followers:.1f} to {upper_followers:.1f}\")\n",
    "print(\"Outliers:\")\n",
    "print(follower_outliers[['user_id', 'followers']])\n",
    "\n",
    "# Detect outliers in posts\n",
    "post_outliers, lower_posts, upper_posts = detect_outliers_iqr(user_stats, 'posts')\n",
    "print(f\"\\nüì± Posts outlier detection:\")\n",
    "print(f\"Valid range: {lower_posts:.1f} to {upper_posts:.1f}\")\n",
    "print(\"Outliers:\")\n",
    "print(post_outliers[['user_id', 'posts']])\n",
    "\n",
    "# Remove outliers\n",
    "clean_user_stats = user_stats[\n",
    "    (user_stats['followers'] >= lower_followers) & \n",
    "    (user_stats['followers'] <= upper_followers) &\n",
    "    (user_stats['posts'] >= lower_posts) & \n",
    "    (user_stats['posts'] <= upper_posts)\n",
    "]\n",
    "\n",
    "print(f\"\\n‚úÖ After removing outliers:\")\n",
    "print(f\"Original shape: {user_stats.shape}\")\n",
    "print(f\"Clean shape: {clean_user_stats.shape}\")\n",
    "print(\"\\nClean data statistics:\")\n",
    "print(clean_user_stats.describe())\n",
    "\n",
    "# Method 2: Z-score method (alternative approach)\n",
    "from scipy import stats\n",
    "\n",
    "print(f\"\\nüìä Alternative: Z-score method\")\n",
    "z_scores_followers = np.abs(stats.zscore(user_stats['followers']))\n",
    "z_threshold = 2.5  # Common threshold\n",
    "\n",
    "outliers_z = user_stats[z_scores_followers > z_threshold]\n",
    "print(f\"Outliers using Z-score (threshold={z_threshold}):\")\n",
    "print(outliers_z[['user_id', 'followers']])\n",
    "\n",
    "# Visualize the data distribution (simple histogram)\n",
    "print(f\"\\nüìà Data distribution summary:\")\n",
    "print(f\"Followers - Min: {user_stats['followers'].min()}, Max: {user_stats['followers'].max()}, Median: {user_stats['followers'].median()}\")\n",
    "print(f\"Posts - Min: {user_stats['posts'].min()}, Max: {user_stats['posts'].max()}, Median: {user_stats['posts'].median()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd4689",
   "metadata": {},
   "source": [
    "## 8. Data Grouping and Aggregation\n",
    "\n",
    "Grouping and aggregation are powerful techniques for summarizing data and finding patterns in networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d779eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Grouping and Aggregation Examples\n",
    "\n",
    "# Create extended dataset with more diverse information\n",
    "extended_user_data = {\n",
    "    'user_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'username': ['alice', 'bob', 'charlie', 'diana', 'eve', 'frank', 'grace', 'henry', 'iris', 'jack'],\n",
    "    'followers': [150, 89, 203, 45, 312, 78, 234, 156, 67, 189],\n",
    "    'following': [98, 156, 87, 178, 92, 145, 67, 189, 123, 145],\n",
    "    'posts': [45, 23, 67, 12, 89, 34, 78, 56, 29, 67],\n",
    "    'account_type': ['personal', 'business', 'personal', 'influencer', 'business', \n",
    "                     'personal', 'influencer', 'business', 'personal', 'influencer'],\n",
    "    'location': ['USA', 'Canada', 'USA', 'UK', 'Germany', 'USA', 'UK', 'Canada', 'France', 'UK'],\n",
    "    'verified': [False, True, False, True, True, False, True, True, False, True]\n",
    "}\n",
    "\n",
    "extended_df = pd.DataFrame(extended_user_data)\n",
    "\n",
    "print(\"üìä GROUPING AND AGGREGATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Extended dataset:\")\n",
    "print(extended_df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Basic grouping by account type\n",
    "print(\"üë• GROUP BY ACCOUNT TYPE\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "account_groups = extended_df.groupby('account_type')\n",
    "print(\"Basic statistics by account type:\")\n",
    "account_stats = account_groups[['followers', 'posts', 'following']].mean().round(2)\n",
    "print(account_stats)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "\n",
    "# Multiple aggregation functions\n",
    "print(\"üìà MULTIPLE AGGREGATION FUNCTIONS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "detailed_stats = extended_df.groupby('account_type').agg({\n",
    "    'followers': ['mean', 'max', 'min', 'count'],\n",
    "    'posts': ['sum', 'mean'],\n",
    "    'following': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"Detailed statistics by account type:\")\n",
    "print(detailed_stats)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "\n",
    "# Group by multiple columns\n",
    "print(\"üåç GROUP BY MULTIPLE COLUMNS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "location_account_stats = extended_df.groupby(['location', 'account_type']).agg({\n",
    "    'followers': 'mean',\n",
    "    'posts': 'sum',\n",
    "    'verified': 'count'  # Count of users\n",
    "}).round(2)\n",
    "\n",
    "print(\"Statistics by location and account type:\")\n",
    "print(location_account_stats)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "\n",
    "# Custom aggregation functions\n",
    "print(\"üîß CUSTOM AGGREGATION\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "def engagement_ratio(group):\n",
    "    return (group['followers'] / group['following']).mean()\n",
    "\n",
    "def activity_score(group):\n",
    "    return (group['posts'] * 0.3 + group['followers'] * 0.001).mean()\n",
    "\n",
    "custom_stats = extended_df.groupby('account_type').apply(lambda x: pd.Series({\n",
    "    'avg_engagement_ratio': engagement_ratio(x),\n",
    "    'activity_score': activity_score(x),\n",
    "    'verification_rate': x['verified'].mean()\n",
    "})).round(3)\n",
    "\n",
    "print(\"Custom metrics by account type:\")\n",
    "print(custom_stats)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# CREATING DERIVED METRICS\n",
    "print(\"‚ö° DERIVED METRICS\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Add calculated columns\n",
    "extended_df['engagement_ratio'] = extended_df['followers'] / extended_df['following']\n",
    "extended_df['posts_per_follower'] = extended_df['posts'] / extended_df['followers'] * 1000  # per 1000 followers\n",
    "extended_df['influence_score'] = (\n",
    "    extended_df['followers'] * 0.4 + \n",
    "    extended_df['posts'] * 10 + \n",
    "    extended_df['verified'].astype(int) * 50\n",
    ")\n",
    "\n",
    "print(\"Dataset with derived metrics:\")\n",
    "print(extended_df[['username', 'account_type', 'engagement_ratio', 'posts_per_follower', 'influence_score']].round(2))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "\n",
    "# Analyze derived metrics by groups\n",
    "print(\"üìä Analysis of derived metrics:\")\n",
    "derived_analysis = extended_df.groupby('account_type')[['engagement_ratio', 'posts_per_follower', 'influence_score']].agg(['mean', 'std']).round(3)\n",
    "print(derived_analysis)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "\n",
    "# Find top performers in each category\n",
    "print(\"üèÜ TOP PERFORMERS BY CATEGORY\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for account_type in extended_df['account_type'].unique():\n",
    "    top_user = extended_df[extended_df['account_type'] == account_type].nlargest(1, 'influence_score')\n",
    "    print(f\"{account_type.title()}: {top_user['username'].iloc[0]} (Score: {top_user['influence_score'].iloc[0]:.1f})\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "\n",
    "# Verification analysis\n",
    "print(\"‚úÖ VERIFICATION ANALYSIS\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "verification_stats = extended_df.groupby('verified').agg({\n",
    "    'followers': ['mean', 'median'],\n",
    "    'posts': 'mean',\n",
    "    'influence_score': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"Verified vs Non-verified users:\")\n",
    "print(verification_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e6aad0",
   "metadata": {},
   "source": [
    "## 9. Creating Pivot Tables for Network Analysis\n",
    "\n",
    "Pivot tables are excellent for creating adjacency matrices and analyzing network relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe748386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot Tables for Network Analysis\n",
    "\n",
    "print(\"üîÑ PIVOT TABLES FOR NETWORK ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample interaction data\n",
    "interactions = pd.DataFrame({\n",
    "    'from_user': ['alice', 'alice', 'bob', 'bob', 'charlie', 'charlie', 'diana', 'diana', 'eve', 'frank'],\n",
    "    'to_user': ['bob', 'charlie', 'alice', 'charlie', 'alice', 'bob', 'eve', 'frank', 'alice', 'bob'],\n",
    "    'interaction_type': ['like', 'comment', 'share', 'like', 'comment', 'like', 'follow', 'like', 'share', 'comment'],\n",
    "    'timestamp': pd.date_range('2025-01-01', periods=10, freq='6H'),\n",
    "    'weight': [1, 2, 1, 1, 3, 1, 1, 1, 2, 1]  # interaction strength\n",
    "})\n",
    "\n",
    "print(\"Sample interaction data:\")\n",
    "print(interactions)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Create basic adjacency matrix using pivot table\n",
    "print(\"üîó BASIC ADJACENCY MATRIX\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Count of interactions between users\n",
    "adjacency_matrix = interactions.pivot_table(\n",
    "    index='from_user',\n",
    "    columns='to_user',\n",
    "    values='interaction_type',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(\"Adjacency matrix (interaction counts):\")\n",
    "print(adjacency_matrix)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "\n",
    "# Weighted adjacency matrix\n",
    "print(\"‚öñÔ∏è WEIGHTED ADJACENCY MATRIX\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "weighted_matrix = interactions.pivot_table(\n",
    "    index='from_user',\n",
    "    columns='to_user',\n",
    "    values='weight',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(\"Weighted adjacency matrix (sum of weights):\")\n",
    "print(weighted_matrix)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "\n",
    "# Interaction type breakdown\n",
    "print(\"üìä INTERACTION TYPE BREAKDOWN\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "interaction_breakdown = interactions.pivot_table(\n",
    "    index='interaction_type',\n",
    "    columns='from_user',\n",
    "    values='weight',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(\"Interactions by type and user:\")\n",
    "print(interaction_breakdown)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# More complex network analysis\n",
    "print(\"üéØ ADVANCED NETWORK METRICS\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "# Calculate in-degree and out-degree\n",
    "out_degree = interactions.groupby('from_user')['to_user'].count()\n",
    "in_degree = interactions.groupby('to_user')['from_user'].count()\n",
    "\n",
    "# Combine into a network metrics dataframe\n",
    "all_users = set(interactions['from_user'].unique()) | set(interactions['to_user'].unique())\n",
    "network_metrics = pd.DataFrame(index=sorted(all_users))\n",
    "network_metrics['out_degree'] = out_degree\n",
    "network_metrics['in_degree'] = in_degree\n",
    "network_metrics = network_metrics.fillna(0).astype(int)\n",
    "network_metrics['total_degree'] = network_metrics['out_degree'] + network_metrics['in_degree']\n",
    "\n",
    "print(\"Network metrics by user:\")\n",
    "print(network_metrics)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "\n",
    "# Time-based analysis\n",
    "print(\"‚è∞ TIME-BASED INTERACTION ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Add hour and day information\n",
    "interactions['hour'] = interactions['timestamp'].dt.hour\n",
    "interactions['day'] = interactions['timestamp'].dt.day\n",
    "\n",
    "# Pivot by time\n",
    "hourly_activity = interactions.pivot_table(\n",
    "    index='hour',\n",
    "    columns='interaction_type',\n",
    "    values='weight',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(\"Activity by hour and interaction type:\")\n",
    "print(hourly_activity)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "\n",
    "# Cross-tabulation (alternative to pivot table)\n",
    "print(\"üìã CROSS-TABULATION EXAMPLE\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "# Create cross-tab of user interactions\n",
    "crosstab = pd.crosstab(\n",
    "    interactions['from_user'], \n",
    "    interactions['interaction_type'], \n",
    "    margins=True\n",
    ")\n",
    "\n",
    "print(\"Cross-tabulation of users and interaction types:\")\n",
    "print(crosstab)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Convert to NetworkX format (preview for later weeks)\n",
    "print(\"üï∏Ô∏è PREVIEW: NETWORKX INTEGRATION\")\n",
    "print(\"-\" * 38)\n",
    "\n",
    "# Create edge list format\n",
    "edge_list = interactions[['from_user', 'to_user', 'weight']].copy()\n",
    "edge_list.columns = ['source', 'target', 'weight']\n",
    "\n",
    "print(\"Edge list format (ready for NetworkX):\")\n",
    "print(edge_list)\n",
    "\n",
    "# Basic network statistics\n",
    "unique_nodes = len(set(edge_list['source']) | set(edge_list['target']))\n",
    "total_edges = len(edge_list)\n",
    "max_possible_edges = unique_nodes * (unique_nodes - 1)\n",
    "density = total_edges / max_possible_edges if max_possible_edges > 0 else 0\n",
    "\n",
    "print(f\"\\nBasic network statistics:\")\n",
    "print(f\"  Nodes: {unique_nodes}\")\n",
    "print(f\"  Edges: {total_edges}\")\n",
    "print(f\"  Density: {density:.3f}\")\n",
    "print(f\"  Average degree: {(total_edges * 2) / unique_nodes:.2f}\")\n",
    "\n",
    "# Most active users\n",
    "print(f\"\\nMost active users (by total interactions):\")\n",
    "user_activity = interactions.groupby('from_user').size().sort_values(ascending=False)\n",
    "print(user_activity.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58acb507",
   "metadata": {},
   "source": [
    "## 10. Data Visualization Basics\n",
    "\n",
    "Visualization helps us understand patterns in network data. Let's create basic plots for network analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization for Network Analysis\n",
    "\n",
    "# Set up matplotlib for better plots\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"üìä DATA VISUALIZATION FOR NETWORKS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use our extended dataset for visualizations\n",
    "viz_data = extended_df.copy()\n",
    "\n",
    "# 1. HISTOGRAMS - Distribution of followers\n",
    "print(\"üìà Creating visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Social Network Data Visualizations', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Histogram of followers\n",
    "axes[0, 0].hist(viz_data['followers'], bins=8, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Distribution of Followers')\n",
    "axes[0, 0].set_xlabel('Number of Followers')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bar chart by account type\n",
    "account_counts = viz_data['account_type'].value_counts()\n",
    "axes[0, 1].bar(account_counts.index, account_counts.values, \n",
    "               color=['lightcoral', 'lightblue', 'lightgreen'])\n",
    "axes[0, 1].set_title('Users by Account Type')\n",
    "axes[0, 1].set_xlabel('Account Type')\n",
    "axes[0, 1].set_ylabel('Number of Users')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Scatter plot: Followers vs Posts\n",
    "colors = {'personal': 'blue', 'business': 'red', 'influencer': 'green'}\n",
    "for account_type in viz_data['account_type'].unique():\n",
    "    data_subset = viz_data[viz_data['account_type'] == account_type]\n",
    "    axes[1, 0].scatter(data_subset['followers'], data_subset['posts'], \n",
    "                       c=colors[account_type], label=account_type, alpha=0.7, s=50)\n",
    "\n",
    "axes[1, 0].set_title('Followers vs Posts by Account Type')\n",
    "axes[1, 0].set_xlabel('Number of Followers')\n",
    "axes[1, 0].set_ylabel('Number of Posts')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot: Followers by location\n",
    "location_data = [viz_data[viz_data['location'] == loc]['followers'].values \n",
    "                 for loc in viz_data['location'].unique()]\n",
    "axes[1, 1].boxplot(location_data, labels=viz_data['location'].unique())\n",
    "axes[1, 1].set_title('Followers Distribution by Location')\n",
    "axes[1, 1].set_xlabel('Location')\n",
    "axes[1, 1].set_ylabel('Number of Followers')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Basic plots created!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# 2. SEABORN PLOTS - More advanced visualizations\n",
    "print(\"üé® ADVANCED VISUALIZATIONS WITH SEABORN\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Create figure with seaborn plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Advanced Network Data Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Heatmap of correlation matrix\n",
    "numeric_cols = ['followers', 'following', 'posts', 'engagement_ratio']\n",
    "correlation_matrix = viz_data[numeric_cols].corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Correlation Matrix')\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(data=viz_data, x='account_type', y='followers', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Followers Distribution by Account Type')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Count plot with hue\n",
    "sns.countplot(data=viz_data, x='location', hue='verified', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Verified vs Non-verified by Location')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pair plot data (using subset for clarity)\n",
    "pair_data = viz_data[['followers', 'posts', 'account_type']].copy()\n",
    "# Create a simple scatter for the subplot\n",
    "for i, account_type in enumerate(viz_data['account_type'].unique()):\n",
    "    data_subset = viz_data[viz_data['account_type'] == account_type]\n",
    "    axes[1, 1].scatter(data_subset['followers'], data_subset['engagement_ratio'], \n",
    "                       label=account_type, alpha=0.7)\n",
    "axes[1, 1].set_title('Engagement Ratio vs Followers')\n",
    "axes[1, 1].set_xlabel('Followers')\n",
    "axes[1, 1].set_ylabel('Engagement Ratio')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Advanced plots created!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# 3. NETWORK-SPECIFIC VISUALIZATIONS\n",
    "print(\"üï∏Ô∏è NETWORK-SPECIFIC VISUALIZATIONS\")\n",
    "print(\"-\" * 38)\n",
    "\n",
    "# Create adjacency matrix heatmap\n",
    "print(\"Creating adjacency matrix heatmap...\")\n",
    "\n",
    "# Use interaction data from previous section\n",
    "interaction_matrix = pd.crosstab(interactions['from_user'], interactions['to_user'])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(interaction_matrix, annot=True, cmap='Blues', \n",
    "            square=True, cbar_kws={'label': 'Number of Interactions'})\n",
    "plt.title('User Interaction Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('To User')\n",
    "plt.ylabel('From User')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "\n",
    "# Degree distribution\n",
    "print(\"Creating degree distribution plot...\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot degree distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "degree_dist = network_metrics['total_degree'].value_counts().sort_index()\n",
    "plt.bar(degree_dist.index, degree_dist.values, alpha=0.7, color='orange')\n",
    "plt.title('Degree Distribution')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Number of Nodes')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot in-degree vs out-degree\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(network_metrics['in_degree'], network_metrics['out_degree'], \n",
    "           alpha=0.7, s=100, color='purple')\n",
    "for i, user in enumerate(network_metrics.index):\n",
    "    plt.annotate(user, (network_metrics['in_degree'].iloc[i], \n",
    "                        network_metrics['out_degree'].iloc[i]),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "plt.title('In-degree vs Out-degree')\n",
    "plt.xlabel('In-degree')\n",
    "plt.ylabel('Out-degree')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Network visualizations created!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Summary statistics for visualization\n",
    "print(\"üìä VISUALIZATION INSIGHTS\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "print(f\"Data insights from visualizations:\")\n",
    "print(f\"  üìà Follower range: {viz_data['followers'].min()} - {viz_data['followers'].max()}\")\n",
    "print(f\"  üìä Most common account type: {viz_data['account_type'].mode().iloc[0]}\")\n",
    "print(f\"  üåç Most represented location: {viz_data['location'].mode().iloc[0]}\")\n",
    "print(f\"  ‚úÖ Verification rate: {viz_data['verified'].mean():.1%}\")\n",
    "print(f\"  üîó Average engagement ratio: {viz_data['engagement_ratio'].mean():.2f}\")\n",
    "\n",
    "# Correlation insights\n",
    "strongest_corr = correlation_matrix.abs().unstack().sort_values(ascending=False)\n",
    "# Remove self-correlations\n",
    "strongest_corr = strongest_corr[strongest_corr < 1.0]\n",
    "print(f\"  üîó Strongest correlation: {strongest_corr.index[0]} ({strongest_corr.iloc[0]:.3f})\")\n",
    "\n",
    "print(f\"\\nüí° Network insights:\")\n",
    "print(f\"  üë• Most connected user: {network_metrics.loc[network_metrics['total_degree'].idxmax()].name}\")\n",
    "print(f\"  üì§ Highest out-degree: {network_metrics['out_degree'].max()}\")\n",
    "print(f\"  üì• Highest in-degree: {network_metrics['in_degree'].max()}\")\n",
    "print(f\"  üéØ Average degree: {network_metrics['total_degree'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398eb974",
   "metadata": {},
   "source": [
    "## 11. Real-world Dataset Exercise\n",
    "\n",
    "Let's apply everything we've learned to a comprehensive real-world example that combines all the techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcf8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Real-world Dataset Exercise\n",
    "\n",
    "print(\"üåç REAL-WORLD DATASET EXERCISE\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Simulating a Twitter-like social network dataset with realistic challenges\")\n",
    "\n",
    "# Create a more complex, realistic dataset\n",
    "np.random.seed(42)  # For reproducible results\n",
    "\n",
    "# Generate realistic social media data\n",
    "n_users = 50\n",
    "user_ids = range(1, n_users + 1)\n",
    "\n",
    "# Generate user data with realistic patterns\n",
    "user_data = []\n",
    "for i in user_ids:\n",
    "    # Account types with realistic distributions\n",
    "    account_type = np.random.choice(['personal', 'business', 'influencer'], \n",
    "                                   p=[0.7, 0.2, 0.1])\n",
    "    \n",
    "    # Followers based on account type\n",
    "    if account_type == 'influencer':\n",
    "        followers = np.random.randint(1000, 10000)\n",
    "    elif account_type == 'business':\n",
    "        followers = np.random.randint(100, 2000)\n",
    "    else:  # personal\n",
    "        followers = np.random.randint(10, 500)\n",
    "    \n",
    "    # Following count (influencers follow fewer people proportionally)\n",
    "    if account_type == 'influencer':\n",
    "        following = np.random.randint(50, min(500, followers // 5))\n",
    "    else:\n",
    "        following = np.random.randint(20, min(800, followers))\n",
    "    \n",
    "    # Posts (more variable)\n",
    "    posts = np.random.randint(5, 200)\n",
    "    \n",
    "    # Location distribution\n",
    "    location = np.random.choice(['USA', 'UK', 'Canada', 'Germany', 'France', 'Australia'], \n",
    "                               p=[0.4, 0.15, 0.1, 0.1, 0.1, 0.15])\n",
    "    \n",
    "    # Verification (more likely for businesses and influencers)\n",
    "    if account_type == 'influencer':\n",
    "        verified = np.random.choice([True, False], p=[0.8, 0.2])\n",
    "    elif account_type == 'business':\n",
    "        verified = np.random.choice([True, False], p=[0.4, 0.6])\n",
    "    else:\n",
    "        verified = np.random.choice([True, False], p=[0.05, 0.95])\n",
    "    \n",
    "    # Join date (some variation)\n",
    "    join_date = pd.Timestamp('2020-01-01') + pd.Timedelta(days=np.random.randint(0, 1500))\n",
    "    \n",
    "    user_data.append({\n",
    "        'user_id': i,\n",
    "        'username': f'user_{i:03d}',\n",
    "        'followers': followers,\n",
    "        'following': following,\n",
    "        'posts': posts,\n",
    "        'account_type': account_type,\n",
    "        'location': location,\n",
    "        'verified': verified,\n",
    "        'join_date': join_date\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "social_network_df = pd.DataFrame(user_data)\n",
    "\n",
    "print(\"üìä Generated realistic social network dataset:\")\n",
    "print(f\"Shape: {social_network_df.shape}\")\n",
    "print(social_network_df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# STEP 1: DATA EXPLORATION\n",
    "print(\"üîç STEP 1: COMPREHENSIVE DATA EXPLORATION\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "print(\"Basic information:\")\n",
    "print(social_network_df.info())\n",
    "\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  üìä Total users: {len(social_network_df)}\")\n",
    "print(f\"  üìÖ Date range: {social_network_df['join_date'].min().date()} to {social_network_df['join_date'].max().date()}\")\n",
    "print(f\"  üåç Locations: {social_network_df['location'].nunique()}\")\n",
    "print(f\"  üë• Account types: {social_network_df['account_type'].nunique()}\")\n",
    "\n",
    "# Check for data quality issues\n",
    "print(f\"\\nüîç Data quality check:\")\n",
    "print(f\"  Missing values: {social_network_df.isnull().sum().sum()}\")\n",
    "print(f\"  Duplicate usernames: {social_network_df['username'].duplicated().sum()}\")\n",
    "print(f\"  Impossible ratios (following > followers * 10): {(social_network_df['following'] > social_network_df['followers'] * 10).sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 45)\n",
    "\n",
    "# STEP 2: DATA CLEANING AND PREPROCESSING\n",
    "print(\"üßπ STEP 2: DATA CLEANING AND PREPROCESSING\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Add some realistic missing values and errors\n",
    "clean_df = social_network_df.copy()\n",
    "\n",
    "# Introduce some missing values (realistic scenario)\n",
    "missing_indices = np.random.choice(clean_df.index, size=5, replace=False)\n",
    "clean_df.loc[missing_indices[:3], 'location'] = np.nan\n",
    "clean_df.loc[missing_indices[3:], 'posts'] = np.nan\n",
    "\n",
    "# Add some duplicate users (data entry errors)\n",
    "duplicate_user = clean_df.iloc[0].copy()\n",
    "duplicate_user['user_id'] = len(clean_df) + 1\n",
    "duplicate_user['username'] = clean_df.iloc[0]['username']  # Same username\n",
    "clean_df = pd.concat([clean_df, duplicate_user.to_frame().T], ignore_index=True)\n",
    "\n",
    "print(\"Data after introducing realistic issues:\")\n",
    "print(f\"  Missing values: {clean_df.isnull().sum().sum()}\")\n",
    "print(f\"  Duplicate usernames: {clean_df['username'].duplicated().sum()}\")\n",
    "\n",
    "# Clean the data\n",
    "print(f\"\\nCleaning process:\")\n",
    "\n",
    "# Handle missing values\n",
    "print(\"  üìç Filling missing locations with 'Unknown'\")\n",
    "clean_df['location'].fillna('Unknown', inplace=True)\n",
    "\n",
    "print(\"  üì± Filling missing posts with median by account type\")\n",
    "for account_type in clean_df['account_type'].unique():\n",
    "    median_posts = clean_df[clean_df['account_type'] == account_type]['posts'].median()\n",
    "    mask = (clean_df['account_type'] == account_type) & (clean_df['posts'].isna())\n",
    "    clean_df.loc[mask, 'posts'] = median_posts\n",
    "\n",
    "# Remove duplicates\n",
    "print(\"  üóëÔ∏è Removing duplicate usernames\")\n",
    "clean_df = clean_df.drop_duplicates(subset=['username'], keep='first')\n",
    "\n",
    "print(f\"\\nAfter cleaning:\")\n",
    "print(f\"  Shape: {clean_df.shape}\")\n",
    "print(f\"  Missing values: {clean_df.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 45)\n",
    "\n",
    "# STEP 3: FEATURE ENGINEERING\n",
    "print(\"‚öôÔ∏è STEP 3: FEATURE ENGINEERING\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Create derived features\n",
    "clean_df['engagement_ratio'] = clean_df['followers'] / clean_df['following'].replace(0, 1)\n",
    "clean_df['posts_per_follower'] = clean_df['posts'] / clean_df['followers'].replace(0, 1) * 1000\n",
    "clean_df['account_age_days'] = (pd.Timestamp.now() - clean_df['join_date']).dt.days\n",
    "clean_df['influence_score'] = (\n",
    "    clean_df['followers'] * 0.4 + \n",
    "    clean_df['posts'] * 5 + \n",
    "    clean_df['verified'].astype(int) * 100 +\n",
    "    clean_df['account_age_days'] * 0.1\n",
    ")\n",
    "\n",
    "# Categorize users by activity level\n",
    "clean_df['activity_level'] = pd.cut(clean_df['posts'], \n",
    "                                   bins=[0, 25, 75, 200], \n",
    "                                   labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(\"‚úÖ Created derived features:\")\n",
    "print(\"  - engagement_ratio\")\n",
    "print(\"  - posts_per_follower\")\n",
    "print(\"  - account_age_days\")\n",
    "print(\"  - influence_score\")\n",
    "print(\"  - activity_level\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 45)\n",
    "\n",
    "# STEP 4: COMPREHENSIVE ANALYSIS\n",
    "print(\"üìä STEP 4: COMPREHENSIVE ANALYSIS\")\n",
    "print(\"-\" * 38)\n",
    "\n",
    "# Analysis by account type\n",
    "print(\"Analysis by account type:\")\n",
    "account_analysis = clean_df.groupby('account_type').agg({\n",
    "    'followers': ['mean', 'median', 'max'],\n",
    "    'posts': 'mean',\n",
    "    'engagement_ratio': 'mean',\n",
    "    'influence_score': 'mean',\n",
    "    'verified': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(account_analysis)\n",
    "\n",
    "print(f\"\\nüèÜ Top 5 most influential users:\")\n",
    "top_influencers = clean_df.nlargest(5, 'influence_score')[['username', 'account_type', 'followers', 'influence_score']]\n",
    "for idx, user in top_influencers.iterrows():\n",
    "    print(f\"  {user['username']}: {user['influence_score']:.0f} ({user['account_type']}, {user['followers']} followers)\")\n",
    "\n",
    "print(f\"\\nüåç Geographic distribution:\")\n",
    "geo_dist = clean_df['location'].value_counts()\n",
    "for location, count in geo_dist.items():\n",
    "    percentage = count / len(clean_df) * 100\n",
    "    print(f\"  {location}: {count} users ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìà Activity level distribution:\")\n",
    "activity_dist = clean_df['activity_level'].value_counts()\n",
    "for level, count in activity_dist.items():\n",
    "    percentage = count / len(clean_df) * 100\n",
    "    print(f\"  {level}: {count} users ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 45)\n",
    "\n",
    "# STEP 5: INSIGHTS AND RECOMMENDATIONS\n",
    "print(\"üí° STEP 5: KEY INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Calculate key metrics\n",
    "avg_engagement = clean_df['engagement_ratio'].mean()\n",
    "verification_rate = clean_df['verified'].mean()\n",
    "median_followers = clean_df['followers'].median()\n",
    "\n",
    "print(\"Key insights:\")\n",
    "print(f\"  üìä Average engagement ratio: {avg_engagement:.2f}\")\n",
    "print(f\"  ‚úÖ Overall verification rate: {verification_rate:.1%}\")\n",
    "print(f\"  üë• Median followers: {median_followers:.0f}\")\n",
    "\n",
    "# Business insights\n",
    "business_users = clean_df[clean_df['account_type'] == 'business']\n",
    "influencer_users = clean_df[clean_df['account_type'] == 'influencer']\n",
    "\n",
    "print(f\"\\nüè¢ Business account insights:\")\n",
    "print(f\"  Average followers: {business_users['followers'].mean():.0f}\")\n",
    "print(f\"  Verification rate: {business_users['verified'].mean():.1%}\")\n",
    "print(f\"  Most active location: {business_users['location'].mode().iloc[0]}\")\n",
    "\n",
    "print(f\"\\n‚≠ê Influencer account insights:\")\n",
    "print(f\"  Average followers: {influencer_users['followers'].mean():.0f}\")\n",
    "print(f\"  Verification rate: {influencer_users['verified'].mean():.1%}\")\n",
    "print(f\"  Average engagement ratio: {influencer_users['engagement_ratio'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nüìã Recommendations for platform:\")\n",
    "print(\"  1. Focus verification efforts on influencers and businesses\")\n",
    "print(\"  2. Encourage content creation among low-activity users\")\n",
    "print(\"  3. Develop location-specific features for major markets\")\n",
    "print(\"  4. Monitor high-engagement-ratio accounts for potential bot activity\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üéØ EXERCISE COMPLETED!\")\n",
    "print(\"This comprehensive analysis demonstrates all the pandas skills\")\n",
    "print(\"we've learned in this session. You can apply these techniques\")\n",
    "print(\"to any real-world dataset in your homework and projects!\")\n",
    "\n",
    "# Final dataset summary\n",
    "print(f\"\\nFinal dataset summary:\")\n",
    "print(f\"  üìä Users analyzed: {len(clean_df)}\")\n",
    "print(f\"  üîß Features created: {len(clean_df.columns)}\")\n",
    "print(f\"  üìà Insights generated: Multiple business-relevant findings\")\n",
    "print(f\"  üéØ Ready for: Machine learning and network analysis (upcoming weeks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae98886",
   "metadata": {},
   "source": [
    "## üéØ Today's Summary and Next Steps\n",
    "\n",
    "### What We Accomplished Today\n",
    "‚úÖ **Environment Setup:** Configured Python environment and imported essential libraries  \n",
    "‚úÖ **Python Review:** Refreshed fundamental programming concepts  \n",
    "‚úÖ **DataFrame Mastery:** Created, explored, and manipulated pandas DataFrames  \n",
    "‚úÖ **Data Cleaning:** Handled missing values, duplicates, and outliers  \n",
    "‚úÖ **Data Analysis:** Performed grouping, aggregation, and pivot table operations  \n",
    "‚úÖ **Visualization:** Created basic plots for network data  \n",
    "‚úÖ **Real-world Application:** Applied all techniques to a comprehensive dataset\n",
    "\n",
    "### Lab Activity: Clean and Summarize Real Data\n",
    "**Objective:** Apply today's techniques to a provided social network dataset\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the provided Twitter interaction dataset\n",
    "2. Explore data structure and identify quality issues\n",
    "3. Clean the data (missing values, duplicates, outliers)\n",
    "4. Create summary statistics and visualizations\n",
    "5. Generate insights about network structure\n",
    "\n",
    "**Deliverable:** Jupyter notebook with clean code, comments, and insights\n",
    "\n",
    "### Homework 1 Assignment\n",
    "**Due:** Next week before class  \n",
    "**Task:** Explore and clean a dataset from UCI ML Repository or Kaggle\n",
    "\n",
    "**Requirements:**\n",
    "- Choose a dataset with network/social components\n",
    "- Perform comprehensive data cleaning and exploration\n",
    "- Compute at least 5 meaningful statistics\n",
    "- Create 3-4 visualizations\n",
    "- Write a 1-page summary of findings and data quality issues\n",
    "\n",
    "**Submission:** Jupyter notebook + PDF summary via course portal\n",
    "\n",
    "### Looking Ahead: Week 2 - Supervised Learning\n",
    "- Train/test splits and cross-validation\n",
    "- Decision trees and k-nearest neighbors  \n",
    "- Evaluation metrics: accuracy, precision, recall, F1-score\n",
    "- Building classifiers for network problems\n",
    "\n",
    "### Resources for Success\n",
    "- **Documentation:** pandas.pydata.org, numpy.org, matplotlib.org, seaborn.pydata.org\n",
    "- **Getting Help:** Office hours, discussion forum, study groups\n",
    "- **Best Practices:** Comment code, use meaningful names, test with small examples\n",
    "\n",
    "### Questions? \n",
    "Feel free to ask during lab time or reach out during office hours!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
